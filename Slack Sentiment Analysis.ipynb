{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-contact",
   "metadata": {},
   "source": [
    "# Slack Sentiment Analysis\n",
    "\n",
    "This notebook is a somewhat pointless but simple exercise where we use [Natural Language Toolkit](https://www.nltk.org) models, both pre-trained and trained by us, to figure out if the general sentiment in a slack channel is positive or negative assuming (most likely incorrectly) that we can judge such a thing using amazon, yelp and movie reviews as examples of positive or negative chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-jamaica",
   "metadata": {},
   "source": [
    "## Fetch Slack Data\n",
    "\n",
    "\n",
    ">If you do not want to use a slack channel you don't have to. If you skip ahead to **\"Prepare the Data\"** you will find some code where you can comment out a bit to use sample data provided by NLTK instead.\n",
    "\n",
    "To get messages from a slack channel you will need a token that has permissions to read from that channel as well as the ID of the channel. \n",
    "\n",
    "**To get the slack channel ID:**\n",
    "\n",
    "There is probably an easier way but the one I use is to right click the channel name in the channel menu and select \"Copy link\". The link will look something like <https://your-org.slack.com/archives/C12345>. The channel ID is the last piece. That's C12345 in the example URL. \n",
    "\n",
    "**To get a token:**\n",
    "\n",
    "Setup a bot user following these instructions <https://api.slack.com/bot-users>. \n",
    "\n",
    "The OAuth scope must allow `channels:history`. Add the bot user to the channel you wish to read messages from. The channel should be public. I have not tested any of this with private channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-array",
   "metadata": {},
   "source": [
    "The next section assumed you have 2 environment variables:\n",
    "\n",
    "`SLACK_SENTIMENT_BOT_TOKEN` - The bot token we setup previously.\n",
    "\n",
    "`SLACK_SENTIMENT_CHANNEL_ID` - The channel ID we retrieved previously.\n",
    "\n",
    "If you are not familiar with how to setup environment variables , feel free to just replace those values with hardcoded values but obviously be careful not to accidentally push your token up to Github or something like that :)\n",
    "\n",
    "Incidentally, I do have a post on managing secrets while developing if you're interested: [a post on managing secrets](https://dev.to/ruarfff/managing-local-app-secrets-and-sharing-secrets-with-your-team-34m1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import WebClient from Python SDK (github.com/slackapi/python-slack-sdk)\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "token = os.environ.get(\"SLACK_SENTIMENT_BOT_TOKEN\")\n",
    "channel_id = os.environ.get(\"SLACK_SENTIMENT_CHANNEL_ID\")\n",
    "\n",
    "client = WebClient(token=token)\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    result = client.conversations_history(channel=channel_id)\n",
    "    conversation_history = result[\"messages\"]\n",
    "except SlackApiError as e:\n",
    "    print(\"Error getting conversations: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-still",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "We will try a few things out here. We will split out the data into sentences and words using NLTK tokenize methods. We will clean up the data to remove a large amount of noise. \n",
    "\n",
    "There are a lof ot pieces in the text that aren't really useful. Punctuation for example. Also things like emojis and various words that don't hold much meaning out of context (NLTK provdes a set of these called \"stopwords\"). It would actually be useful to include emojis in this kind of analysis but let's keep it simple and exclude them here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import itertools\n",
    "\n",
    "# If you do not want to use a slack channel, uncomment this and remove/comment out the next get_test_data function\n",
    "# def get_test_data():\n",
    "#    download('state_union')\n",
    "#    words = nltk.corpus.state_union.words()\n",
    "#    sentences = sent_tokenize(' '.join(words))\n",
    "#    return (sentences, words)\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    all_text = [message['text'] for message in conversation_history if 'text' in message]\n",
    "    tokenized_text = [sent_tokenize(text) for text in all_text]\n",
    "    sentences = list(itertools.chain(*tokenized_text))\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "    words = list(itertools.chain(*tokenized_words))\n",
    "    return (sentences, words)\n",
    "\n",
    "main_sentences, main_words = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-supplement",
   "metadata": {},
   "source": [
    "## Use a Pre-trained Model\n",
    "\n",
    "Let's look at a fairly simple method to take the data we just prepared and run it through a pre trained model. \n",
    "\n",
    "Later we will look at using all the words from the slack chat with a mixture of frequency distribution and naive bayes to gauge the genral sentiment with a model we train. Here we will instead try going over the chat sentence by sentnece and create a pie chart of Positive VS Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# The SentimentIntensityAnalyzer model needs us to pull down the vader_lexicon\n",
    "download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "def is_positive(sentence: str) -> bool:\n",
    "    return sia.polarity_scores(sentence)[\"compound\"] > 0\n",
    "\n",
    "for s in main_sentences:\n",
    "    if is_positive(s):\n",
    "        num_pos +=1\n",
    "    else:\n",
    "        num_neg += 1\n",
    "        \n",
    "labels = 'Positive', 'Negative'\n",
    "sizes = [num_pos, num_neg]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-dietary",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "From this point on we won't be using the sentences anymore and will focus on the individual words instead. Let's create a frequency distribution graph of the words. This is to demonstrate how many of them are more of less useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(main_words)\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-spirit",
   "metadata": {},
   "source": [
    "When we start training our own model we will be using a method where we store each word along wiht its frequency and train a model using those features (the words and frequency) along with labels (positive and negative).\n",
    "\n",
    "We will need 2 functions. One to clean out data, removing noise such as meaningless words, punctuation etc. that could introduce greater incuracy to our analysis. Then we need a function to convert a stream of words into a hashmap with words and number of occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('stopwords')\n",
    "download('names')\n",
    "\n",
    "from nltk.corpus import stopwords, names\n",
    "from string import punctuation\n",
    "\n",
    "name_words = set([n.lower() for n in names.words()])\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_words(words):\n",
    "    return [w for w in [w.lower() for w in words if w.isalpha()] if w not in stop_words and w not in name_words and w not in punctuation]\n",
    "\n",
    "def word_counts(words):\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_words = clean_words(main_words)\n",
    "main_words_counts = word_counts(main_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-belly",
   "metadata": {},
   "source": [
    "Let's see what effect that had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(main_words)\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-reading",
   "metadata": {},
   "source": [
    "We will come back to those values later. First we will work on training a model from existing labelled data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-saying",
   "metadata": {},
   "source": [
    "## Training Our Own Model\n",
    "\n",
    "Using a pre-trained model works to some degree but isn't very interesting. Let's look at training our own model and using it to classify the data we prepared earlier. \n",
    "\n",
    "We are going to train a model using various data sets with text that is labelled as positive or negative. We are going to do some processing on this data to split it up into words with labels. This is called extracting features.\n",
    "\n",
    "We are going to take some datasets from a couple of places and put them all together to create our features.\n",
    "\n",
    "First let's read some data stored in this repository which was downloaded from <https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences>. These files contain text with a label `0` for negative and `1` for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt', names=['review', 'sentiment'], sep='\\t') \n",
    "imdb = pd.read_csv('sentiment_labelled_sentences/imdb_labelled.txt', names=['review', 'sentiment'], sep='\\t') \n",
    "yelp = pd.read_csv('sentiment_labelled_sentences/yelp_labelled.txt', names=['review', 'sentiment'], sep='\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-handle",
   "metadata": {},
   "source": [
    "NLTK also provides some labelled data in its corpus. Let's grab that too for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-illinois",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "\n",
    "Let's process all the data into 2 collections of positive and negative. We use 2 helper functions that we created earlier. One to clean up the data set. The other to create the hashmap of words with their number of occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [*amazon['review'].values, *imdb['review'].values, *yelp['review'].values]\n",
    "sentiment = [*amazon['sentiment'].values, *imdb['sentiment'].values, *yelp['sentiment'].values]\n",
    "\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Process the corpus data which is split into files with IDs of pos and neg\n",
    "for f in movie_reviews.fileids('pos'):\n",
    "    positive_reviews.append((word_counts(clean_words(movie_reviews.words(f))), 'pos'))\n",
    "for f in movie_reviews.fileids('neg'):\n",
    "    negative_reviews.append((word_counts(clean_words(movie_reviews.words(f))), 'neg'))\n",
    "\n",
    "# Process the data we extracted from files into reviews and sentiment lists with matching indexes  \n",
    "for i, r in enumerate(reviews):\n",
    "    review_words = word_tokenize(r)\n",
    "    if sentiment[i] == 1:\n",
    "        positive_reviews.append((word_counts(clean_words(review_words)), 'pos'))\n",
    "    else:\n",
    "        negative_reviews.append((word_counts(clean_words(review_words)), 'neg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-steal",
   "metadata": {},
   "source": [
    "Now we have all that training data, let's train a model.\n",
    "\n",
    "We will use an 80/20 split which [may or may not be optimal here](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# We will use 80% of the data set for training and 20% for testing\n",
    "split_pct = .80\n",
    "\n",
    "def split_set(review_set):\n",
    "    split = int(len(review_set)*split_pct)\n",
    "    return (review_set[:split], review_set[split:])\n",
    "\n",
    "pos_train, pos_test = split_set(positive_reviews)\n",
    "neg_train, neg_test = split_set(negative_reviews)\n",
    "\n",
    "train_set = pos_train + neg_train\n",
    "test_set = pos_test + neg_test\n",
    "\n",
    "model = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "# We can figure out how well the training went using ouer test set\n",
    "print(100 * accuracy(model, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-identity",
   "metadata": {},
   "source": [
    "We can see ewhat our model thinks are the most informative features. The most informative featues in this case are words that appear far more frequently with the label `pos` or the label `neg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-portfolio",
   "metadata": {},
   "source": [
    "Finally, let's see if our model thinks the general sentiment in the slack channel (or in whatever data set you used) is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classify(main_words_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-citizenship",
   "metadata": {},
   "source": [
    "## Using a Model for Real\n",
    "\n",
    "Now we can save the model. You could push this model up somewhere and pull it in to some other useful location. For an example of how to deploy this model in a web service using docker please see the [Readme for this project](https://github.com/ruarfff/slack-sentiment/blob/main/README.md).\n",
    "\n",
    "You can imagine some process whereby you train a model at some interval or on some event and push the model to some location that gets pulled down for use in some application or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('.models/sentiment_classifier.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-employer",
   "metadata": {},
   "source": [
    "Just to show one way you might take that file and use it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open('.models/sentiment_classifier.pickle', 'rb')\n",
    "model = pickle.load(model_file)\n",
    "model_file.close()\n",
    "\n",
    "print(100 * accuracy(model, test_set))\n",
    "model.show_most_informative_features()\n",
    "model.classify(main_words_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

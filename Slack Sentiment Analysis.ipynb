{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-contact",
   "metadata": {},
   "source": [
    "To get messages from a slack channel you will need a token that had permissions to read from that channel and the ID of the channel. \n",
    "\n",
    "To get the slack channel ID:\n",
    "\n",
    "There is probably an easier way but the one I use is to reight clkiek the channel name in the channel menu and select \"Copy link\". The link will look something like <https://your-org.slack.com/archives/C12345>. The channel ID is the last piece. That's C12345 in the example URL. \n",
    "\n",
    "To get a token:\n",
    "\n",
    "Setup a bot user following these instructions <https://api.slack.com/bot-users>. \n",
    "\n",
    "Make sure you set the OAuth scope on the bot to allow `channels:history`. Add the bot user to the channel you wish to read messages from. The channel should be public. I have not tested any of this with private channels. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-array",
   "metadata": {},
   "source": [
    "The next section assumed you have 2 environment variables:\n",
    "\n",
    "`SLACK_SENTIMENT_BOT_TOKEN` - the bot token we setup previously\n",
    "`SLACK_SENTIMENT_CHANNEL_ID` - The channel ID we retrieved previously\n",
    "\n",
    "If you are not familiar with how to setup environment variables, feel free to just replace those values with hardcoded values. Obviously be careful not to accidentally push your token up to Github or somethiug like that :)\n",
    "post on managing secrets\n",
    "If you're interested, I have a [post on managing secrets](https://dev.to/ruarfff/managing-local-app-secrets-and-sharing-secrets-with-your-team-34m1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import WebClient from Python SDK (github.com/slackapi/python-slack-sdk)\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "token = os.environ.get(\"SLACK_SENTIMENT_BOT_TOKEN\")\n",
    "channel_id = os.environ.get(\"SLACK_SENTIMENT_CHANNEL_ID\")\n",
    "\n",
    "client = WebClient(token=token)\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    result = client.conversations_history(channel=channel_id)\n",
    "    conversation_history = result[\"messages\"]\n",
    "except SlackApiError as e:\n",
    "    print(\"Error getting conversations: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-still",
   "metadata": {},
   "source": [
    "We will want to train a model to try and figure out the \"sentiment\" of this slack channel but first let's play around with the data a little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take all the text in our slack channel and tokenize it into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import itertools\n",
    "\n",
    "all_text = [message['text'] for message in conversation_history if 'text' in message]\n",
    "\n",
    "\n",
    "tokenized_text = [sent_tokenize(text) for text in all_text]\n",
    "sentences = list(itertools.chain(*tokenized_text))\n",
    "\n",
    "download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "from random import shuffle\n",
    "\n",
    "num_pos = 0\n",
    "num_neg = 0\n",
    "\n",
    "def is_positive(sentence: str) -> bool:\n",
    "    return sia.polarity_scores(sentence)[\"compound\"] > 0\n",
    "\n",
    "shuffle(sentences)\n",
    "for s in sentences:\n",
    "    if is_positive(s):\n",
    "        num_pos +=1\n",
    "    else:\n",
    "        num_neg += 1\n",
    "        \n",
    "labels = 'Good', 'Bad'\n",
    "sizes = [num_pos, num_neg]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-saying",
   "metadata": {},
   "source": [
    "The previous example works to some degree but isn't very interesting. Let's look at traiing our own model and let's also look at cleaning up the data we're working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-airfare",
   "metadata": {},
   "source": [
    "## Prepare our data\n",
    "\n",
    "Previously we just fed a list of sentences to a pre-trained model We want to train our own model but first we also need to do som ework to clean up our data and reduce the noise.\n",
    "\n",
    "There are a lof ot pieces in the text that aren't really useful. Punctuation for example. Also things like emojis. It would probably be useful to include emojis in this kind of analysis but let's keep it simple and exclude them for this example. The following code breaks up the text in to words and renders a frequency distribution chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import download\n",
    "\n",
    "# If you did not use a slack channel, delete of comment out the next 2 lines\n",
    "tokenized_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "words = list(itertools.chain(*tokenized_words))\n",
    "\n",
    "# If you do not want to use a slack channel you can still use some sample data by uncommenting these lines\n",
    "# download('state_union')\n",
    "# words = nltk.corpus.state_union.words()\n",
    "\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-mustang",
   "metadata": {},
   "source": [
    "There are a lot of words showing up a lot that are not very useful. Let's do some processing. First we download some useful data sets provided to use by nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('stopwords')\n",
    "download('names')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-swing",
   "metadata": {},
   "source": [
    "Now we remove as much noise as possible from our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, names\n",
    "from string import punctuation\n",
    "\n",
    "name_words = set([n.lower() for n in names.words()])\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in words if w.isalpha()]\n",
    "words = [w for w in words if w not in stop_words and w not in name_words and w not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(words)\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-transmission",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "Now we are going to train a model off of various data sets with text that is labelled as positive and negative. We are going to do some processing on this data to split it up into words with labels. This is called extracting features. We will extract features of workd frequencies and labels on the words. \n",
    "\n",
    "The model will then classify any text we give it based on how frequently the various words have previously appeared as positive or negative. Of course this is perhaps not foolproof but it's a model to play around with.\n",
    "\n",
    "We are going to take some datasets from a couple of places and put them all together to create our features.\n",
    "\n",
    "First let's read some data stored in this repository which was downloaded from [this repository](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). These files contain text with a label `0` for negative and `1` for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon = pd.read_csv('sentiment_labelled_sentences/amazon_cells_labelled.txt', names=['review', 'sentiment'], sep='\\t') \n",
    "imdb = pd.read_csv('sentiment_labelled_sentences/imdb_labelled.txt', names=['review', 'sentiment'], sep='\\t') \n",
    "yelp = pd.read_csv('sentiment_labelled_sentences/yelp_labelled.txt', names=['review', 'sentiment'], sep='\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-handle",
   "metadata": {},
   "source": [
    "NLTK also provides some labelled data in its corpus. Let's grab that too for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-illinois",
   "metadata": {},
   "source": [
    "Let's process all our data into 2 collections of positive and negative. We will create 2 helper functions. One to clean up the data set, just like we did with our slack data earlier. The other will sort of create a frequency distribution but to use it in our doce it will create a dictionary with each word and the number of occurences of that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(words):\n",
    "    return [w for w in [w.lower() for w in words if w.isalpha()] if w not in stop_words and w not in name_words and w not in punctuation]\n",
    "\n",
    "def word_counts(words):\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [*amazon['review'].values, *imdb['review'].values, *yelp['review'].values]\n",
    "sentiment = [*amazon['sentiment'].values, *imdb['sentiment'].values, *yelp['sentiment'].values]\n",
    "\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "for f in movie_reviews.fileids('pos'):\n",
    "    positive_reviews.append((word_counts(clean_words(movie_reviews.words(f))), 'pos'))\n",
    "for f in movie_reviews.fileids('neg'):\n",
    "    negative_reviews.append((word_counts(clean_words(movie_reviews.words(f))), 'neg'))\n",
    "\n",
    "for i, r in enumerate(reviews):\n",
    "    review_words = word_tokenize(r)\n",
    "    if sentiment[i] == 0:\n",
    "        negative_reviews.append((word_counts(clean_words(review_words)), 'neg'))\n",
    "    else:\n",
    "        positive_reviews.append((word_counts(clean_words(review_words)), 'pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-steal",
   "metadata": {},
   "source": [
    "Now we have all that training data, let's train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pct = .80\n",
    "\n",
    "def split_set(review_set):\n",
    "    split = int(len(review_set)*split_pct)\n",
    "    return (review_set[:split], review_set[split:])\n",
    "\n",
    "pos_train, pos_test = split_set(positive_reviews)\n",
    "neg_train, neg_test = split_set(negative_reviews)\n",
    "\n",
    "train_set = pos_train + neg_train\n",
    "test_set = pos_test + neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "model = NaiveBayesClassifier.train(train_set)\n",
    "print(100 * accuracy(model, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('sa_classifier.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
